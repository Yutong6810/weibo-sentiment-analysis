# simple_app.py - æç®€ç‰ˆæœ¬ï¼Œç¡®ä¿èƒ½éƒ¨ç½²æˆåŠŸ
import streaml# åœ¨ simple_app.py æœ€å¼€å¤´æ·»åŠ 
import sys
import subprocess
import pkg_resources

# æ£€æŸ¥å¹¶å®‰è£…å¿…è¦åŒ…
required = {
    'scikit-learn': '1.3.2',
    'pandas': '2.0.3',
    'numpy': '1.24.3',
    'jieba': '0.42.1'
}

for package, version in required.items():
    try:
        dist = pkg_resources.get_distribution(package)
        if dist.version != version:
            print(f"æ›´æ–° {package} ä» {dist.version} åˆ° {version}")
            subprocess.check_call([sys.executable, "-m", "pip", "install", f"{package}=={version}"])
    except pkg_resources.DistributionNotFound:
        print(f"å®‰è£… {package}=={version}")
        subprocess.check_call([sys.executable, "-m", "pip", "install", f"{package}=={version}"])

# ç°åœ¨å¯¼å…¥
import pickle
import pandas as pd
import numpy as npit as st
import pickle
import pandas as pd
import re

# è®¾ç½®é¡µé¢
st.set_page_config(page_title="æƒ…æ„Ÿåˆ†æç³»ç»Ÿ", layout="wide")
st.title("ğŸ“Š å¾®åšæƒ…æ„Ÿåˆ†æç³»ç»Ÿ")
st.markdown("---")

# å°è¯•åŠ è½½æ¨¡å‹
@st.cache_resource
def load_model():
    try:
        with open('adaboost_nb_best_model.pkl', 'rb') as f:
            model_info = pickle.load(f)
        return model_info
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

model_info = load_model()

if model_info is None:
    st.error("æ— æ³•åŠ è½½æ¨¡å‹æ–‡ä»¶")
    st.stop()

model = model_info['model']
vectorizer = model_info['vectorizer']

# æ ‡ç­¾æ˜ å°„
LABELS = {0: "å®¢è§‚", 1: "ç§¯æ", 2: "æ¶ˆæ"}

# ==================== è¾…åŠ©å‡½æ•° ====================
def simple_tokenize(text):
    """æç®€åˆ†è¯å‡½æ•°ï¼Œæ›¿ä»£ jieba"""
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ŒæŒ‰ç©ºæ ¼åˆ†è¯
    text_clean = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)
    # æŒ‰å­—ç¬¦åˆ†å‰²ï¼Œä½†ä¿ç•™å¸¸ç”¨è¿æ¥
    words = []
    current_word = ""
    for char in text_clean:
        if char.strip():  # ä¸æ˜¯ç©ºæ ¼
            current_word += char
        else:
            if current_word:
                words.append(current_word)
                current_word = ""
    if current_word:
        words.append(current_word)
    
    # è¿‡æ»¤åœç”¨è¯
    stopwords = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº'}
    words = [w for w in words if w not in stopwords and len(w) > 0]
    
    return ' '.join(words)

# ==================== ä¸»ç•Œé¢ ====================
st.header("ğŸ”¤ æ–‡æœ¬æƒ…æ„Ÿåˆ†æ")

# è¾“å…¥åŒºåŸŸ
user_input = st.text_area(
    "è¯·è¾“å…¥å¾®åšå†…å®¹ï¼š",
    "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå¿ƒæƒ…æ„‰å¿«ï¼",
    height=100
)

if st.button("ğŸš€ åˆ†ææƒ…æ„Ÿ", type="primary"):
    # å¤„ç†æ–‡æœ¬
    processed = simple_tokenize(user_input)
    
    try:
        # å‘é‡åŒ–å¹¶é¢„æµ‹
        features = vectorizer.transform([processed])
        pred = model.predict(features)[0]
        proba = model.predict_proba(features)[0]
        
        # æ˜¾ç¤ºç»“æœ
        sentiment = LABELS.get(pred, "æœªçŸ¥")
        confidence = proba[pred]
        
        st.markdown("---")
        st.subheader("ğŸ“Š åˆ†æç»“æœ")
        
        if sentiment == "ç§¯æ":
            st.success(f"âœ… **æƒ…æ„Ÿï¼š{sentiment}** (ç½®ä¿¡åº¦ï¼š{confidence:.2%})")
        elif sentiment == "æ¶ˆæ":
            st.error(f"âŒ **æƒ…æ„Ÿï¼š{sentiment}** (ç½®ä¿¡åº¦ï¼š{confidence:.2%})")
        else:
            st.info(f"ğŸ“„ **æƒ…æ„Ÿï¼š{sentiment}** (ç½®ä¿¡åº¦ï¼š{confidence:.2%})")
        
        # æ˜¾ç¤ºæ¦‚ç‡
        st.markdown("**å„ç±»åˆ«æ¦‚ç‡ï¼š**")
        for i, prob in enumerate(proba):
            label = LABELS.get(i, f"ç±»åˆ«{i}")
            st.write(f"{label}: {prob:.2%}")
            
    except Exception as e:
        st.error(f"é¢„æµ‹å¤±è´¥: {e}")

# ==================== æ‰¹é‡åˆ†æ ====================
st.markdown("---")
st.header("ğŸ“‹ æ‰¹é‡åˆ†æ")

batch_input = st.text_area(
    "è¾“å…¥å¤šæ¡æ–‡æœ¬ï¼ˆæ¯è¡Œä¸€æ¡ï¼‰",
    "ä»Šå¤©å¾ˆå¼€å¿ƒï¼\nè¿™ä¸ªäº§å“å¾ˆç³Ÿç³•\nå¤©æ°”ä¸é”™",
    height=150
)

if st.button("ğŸ“¥ æ‰¹é‡åˆ†æ", type="secondary"):
    texts = [line.strip() for line in batch_input.split('\n') if line.strip()]
    results = []
    
    with st.spinner(f"æ­£åœ¨åˆ†æ {len(texts)} æ¡æ–‡æœ¬..."):
        for text in texts:
            processed = simple_tokenize(text)
            features = vectorizer.transform([processed])
            pred = model.predict(features)[0]
            sentiment = LABELS.get(pred, "æœªçŸ¥")
            results.append((text[:50] + "..." if len(text) > 50 else text, sentiment))
    
    # æ˜¾ç¤ºç»“æœ
    st.markdown("### æ‰¹é‡åˆ†æç»“æœ")
    for i, (text, sentiment) in enumerate(results, 1):
        st.write(f"{i}. `{text}` â†’ **{sentiment}**")
    
    # ç»Ÿè®¡
    sentiments = [r[1] for r in results]
    st.markdown(f"**ç»Ÿè®¡ï¼š** ç§¯æ: {sentiments.count('ç§¯æ')}, æ¶ˆæ: {sentiments.count('æ¶ˆæ')}, å®¢è§‚: {sentiments.count('å®¢è§‚')}")

# ==================== åº•éƒ¨ä¿¡æ¯ ====================
st.sidebar.header("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")
st.sidebar.info("""
### æ¨¡å‹ä¿¡æ¯
- **ç®—æ³•ï¼š** AdaBoostå¢å¼ºæœ´ç´ è´å¶æ–¯
- **å‡†ç¡®ç‡ï¼š** 94%
- **è®­ç»ƒæ•°æ®ï¼š** 10ä¸‡æ¡å¾®åš
- **åˆ†ç±»ï¼š** ç§¯æ/æ¶ˆæ/å®¢è§‚

### åº”ç”¨åœºæ™¯
- å“ç‰Œå£°èª‰ç›‘æµ‹
- ç¤¾äº¤åª’ä½“èˆ†æƒ…åˆ†æ
- ç”¨æˆ·åé¦ˆæƒ…æ„Ÿåˆ†æ
- å¸‚åœºè°ƒç ”æƒ…æ„Ÿå€¾å‘
""")

st.sidebar.markdown("---")
st.sidebar.caption("äººå·¥æ™ºèƒ½å¯¼è®ºå¤§ä½œä¸š Â· å¾®åšæƒ…æ„Ÿåˆ†æç³»ç»Ÿ")
